{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Keras笑脸识别\n",
    "本部分的任务是读取图片并识别图片上的**笑脸**。\n",
    "Keras是一个比Tensorflow层次更高的框架，一般来说通用的模型都可以通过Keras实现，但是一些复杂的模型在keras中没有，只能用较低层次的Tensorflow或pytorch实现\n",
    "#### 1.1 引入库 and 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import kt_utils \n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# 加载数据\n",
    "X_train_orig,Y_train_orig,X_test,Y_test,classes = kt_utils.load_dataset()\n",
    "X_train = X_train_orig / 255\n",
    "Y_train = Y_train_orig / 255\n",
    "\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 使用keras框架构建训练模型\n",
    "keras与tensorflow不一样的是，我们不需要创建如Z1,A1等的中间变量，只需要保存最后结果，所以在fp的时候只有一个tensor X。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        input_shape - 输入参数的size\n",
    "    返回：\n",
    "        model - keras模型\n",
    "    \"\"\"\n",
    "    # 类似于tensorflow中的创建占位符\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # zero padding\n",
    "    # 由于后面我们要用到7*7的卷积 所以这里p=(7-1)/2=3\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    # Conv - BN - relu\n",
    "    X = Conv2D(32,(7,7),strides=(1,1),name=\"conv0\")(X)\n",
    "    X = BatchNormalization(axis=3,name=\"bn0\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # MaxPooling\n",
    "    X = MaxPooling2D((2,2),name=\"max_pool\")(X)\n",
    "    \n",
    "    # 向量化\n",
    "    X = Flatten()(X)\n",
    "    # Fully Connected\n",
    "    X = Dense(1,activation=\"sigmoid\",name=\"fc\")(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name=\"HappyModel\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 2.0804 - acc: 0.5817\n",
      "Epoch 2/60\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.6351 - acc: 0.7817\n",
      "Epoch 3/60\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3815 - acc: 0.8483\n",
      "Epoch 4/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.3199 - acc: 0.8850\n",
      "Epoch 5/60\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2238 - acc: 0.9200\n",
      "Epoch 6/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1241 - acc: 0.9600\n",
      "Epoch 7/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1171 - acc: 0.9633\n",
      "Epoch 8/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0948 - acc: 0.9650\n",
      "Epoch 9/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0899 - acc: 0.9717\n",
      "Epoch 10/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0709 - acc: 0.9817\n",
      "Epoch 11/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0715 - acc: 0.9800\n",
      "Epoch 12/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0632 - acc: 0.9867\n",
      "Epoch 13/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0699 - acc: 0.9850\n",
      "Epoch 14/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0796 - acc: 0.9733\n",
      "Epoch 15/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0574 - acc: 0.9833\n",
      "Epoch 16/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0543 - acc: 0.9833\n",
      "Epoch 17/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0374 - acc: 0.9917\n",
      "Epoch 18/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0338 - acc: 0.9950\n",
      "Epoch 19/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0312 - acc: 0.9917\n",
      "Epoch 20/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0368 - acc: 0.9883\n",
      "Epoch 21/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0324 - acc: 0.9933\n",
      "Epoch 22/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0286 - acc: 0.9933\n",
      "Epoch 23/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0282 - acc: 0.9917\n",
      "Epoch 24/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0309 - acc: 0.9933\n",
      "Epoch 25/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0272 - acc: 0.9967\n",
      "Epoch 26/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0279 - acc: 0.9967\n",
      "Epoch 27/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0236 - acc: 0.9950\n",
      "Epoch 28/60\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0239 - acc: 0.9967\n",
      "Epoch 29/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0248 - acc: 0.9950\n",
      "Epoch 30/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0191 - acc: 0.9950\n",
      "Epoch 31/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0192 - acc: 0.9967\n",
      "Epoch 32/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0167 - acc: 0.9967\n",
      "Epoch 33/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0185 - acc: 0.9967\n",
      "Epoch 34/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0231 - acc: 0.9950\n",
      "Epoch 35/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0185 - acc: 0.9967\n",
      "Epoch 36/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0186 - acc: 0.9967\n",
      "Epoch 37/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0141 - acc: 0.9983\n",
      "Epoch 38/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0186 - acc: 0.9933\n",
      "Epoch 39/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0190 - acc: 0.9967\n",
      "Epoch 40/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0162 - acc: 0.9967\n",
      "Epoch 41/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0143 - acc: 0.9950\n",
      "Epoch 42/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0131 - acc: 0.9967\n",
      "Epoch 43/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0171 - acc: 0.9933\n",
      "Epoch 44/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 45/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0128 - acc: 0.9967\n",
      "Epoch 46/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 48/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0132 - acc: 0.9967\n",
      "Epoch 49/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0083 - acc: 0.9967\n",
      "Epoch 50/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 51/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0089 - acc: 0.9983\n",
      "Epoch 52/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 54/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0090 - acc: 0.9983\n",
      "Epoch 55/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 56/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0069 - acc: 0.9967\n",
      "Epoch 58/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0074 - acc: 0.9983\n",
      "Epoch 59/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0137 - acc: 0.9950\n",
      "Epoch 60/60\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0079 - acc: 0.9983\n",
      "150/150 [==============================] - 1s 8ms/step\n",
      "Error:5.951823825836182\n",
      "Accuracy:0.6266666674613952\n"
     ]
    }
   ],
   "source": [
    "# 获取模型\n",
    "HappyModel = model(X_train.shape[1:])\n",
    "\n",
    "# 编译模型\n",
    "HappyModel.compile(\"adam\",\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "HappyModel.fit(X_train,Y_train,epochs=60,batch_size=50)\n",
    "\n",
    "# 评估模型\n",
    "preds = HappyModel.evaluate(X_test,Y_test,batch_size=32,verbose=1,sample_weight=None)\n",
    "print(\"Error:\"+str(preds[0]))\n",
    "print(\"Accuracy:\"+str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 利用summary函数观察网络结构\n",
    "HappyModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 利用Keras搭建Resnet\n",
    "思路是：（1）组建基本的**残差块**，（2）将残差块组合起来构建Resnet结构\n",
    "#### 2.1 引入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Tools\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "import resnets_utils \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 构建恒等块(Identity Block)\n",
    "恒等块表示shortcut/skip connection的数据size与经过三次卷积的数据size相等，可以直接相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X,f,filters,stage,block):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X - 输入tensor，维度为(m,n_h_prev,n_w_prev,n_c_prev)\n",
    "        f - 主路径第二个conv层filter的size\n",
    "        filters - 由三个整数组成的list，分别表示三个conv层的filter个数\n",
    "        stage,block - 命名用到的数字\n",
    "    返回：\n",
    "        X - 恒等块的输出\n",
    "    \"\"\"\n",
    "    # 命名\n",
    "    conv_name_base = \"res\"+str(stage)+block+\"_branch\"\n",
    "    bn_name_base = \"bn\"+str(stage)+block+\"_branch\"\n",
    "    \n",
    "    # 获取每一层的filter个数\n",
    "    (F1,F2,F3) = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # 主路径第一层卷积\n",
    "    X = Conv2D(filters=F1,kernel_size=(1,1),strides=(1,1),padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # 主路径第二层卷积\n",
    "    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),padding=\"same\",\n",
    "              name=conv_name_base+\"2b\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # 主路径第三层卷积\n",
    "    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding=\"valid\",\n",
    "              name=conv_name_base+\"2c\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "    \n",
    "    \n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [0.94822985 0.         1.1610144  2.747859   0.         1.36677   ]\n"
     ]
    }
   ],
   "source": [
    "# 测试identity_block\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\",[3,4,4,6])\n",
    "    X = np.random.randn(3,4,4,6)\n",
    "    A = identity_block(A_prev,f=2,filters=[2,4,6],stage=1,block=\"a\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    out = session.run([A],feed_dict={A_prev:X,K.learning_phase():0})\n",
    "    print(\"out = \"+str(out[0][1][1][0]))\n",
    "    \n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 卷积块（convolutional block）\n",
    "卷积块使用于shortcut路径上的数据size与三层卷积后的数据size不同的情况，这时需要在shortcut上执行一次卷积操作，使size相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X,f,filters,stage,block,s=2):\n",
    "    conv_name_base = \"res\"+str(stage)+block+\"_branch\"\n",
    "    bn_name_base = \"bn\"+str(stage)+block+\"_branch\"\n",
    "    \n",
    "    (F1,F2,F3) = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # 主路径第一层卷积\n",
    "    X = Conv2D(filters=F1,kernel_size=(1,1),strides=(s,s),padding=\"valid\",\n",
    "              name=conv_name_base+\"2a\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # 主路径第二层卷积\n",
    "    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),padding=\"same\",\n",
    "              name=conv_name_base+\"2b\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # 主路径第三层卷积\n",
    "    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding=\"valid\",\n",
    "              name=conv_name_base+\"2c\",kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(filters=F3,kernel_size=(1,1),strides=(s,s),padding=\"valid\",\n",
    "                       name=conv_name_base+\"1\",kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3,name=bn_name_base+\"1\")(X)\n",
    "    \n",
    "    X = Add()([X_shortcut,X])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [0.2572376  0.13451828 0.61327505 0.14167278 1.1710308  0.08300225]\n"
     ]
    }
   ],
   "source": [
    "# 测试convolutional block\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\",[3,4,4,6])\n",
    "    X = np.random.randn(3,4,4,6)\n",
    "    A = convolutional_block(A_prev,f=2,filters=[2,4,6],stage=1,block=\"a\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    out = session.run([A],feed_dict={A_prev:X,K.learning_phase():0})\n",
    "    print(\"out = \"+str(out[0][1][1][0]))\n",
    "    \n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 有残差块和恒等块构建Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(64,64,3),classes=6):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # zero padding\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    # stage 1\n",
    "    X = Conv2D(filters=64,kernel_size=(7,7),strides=(2,2),name=\"conv1\",\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = MaxPooling2D(pool_size=(3,3),strides=(2,2))(X)\n",
    "    \n",
    "    # stage 2#stage2\n",
    "    X = convolutional_block(X, f=3, filters=[64,64,256], stage=2, block=\"a\", s=1)\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"c\")\n",
    "    \n",
    "    #stage3\n",
    "    X = convolutional_block(X, f=3, filters=[128,128,512], stage=3, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"d\")\n",
    "    \n",
    "    #stage4\n",
    "    X = convolutional_block(X, f=3, filters=[256,256,1024], stage=4, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"d\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"e\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"f\")\n",
    "    \n",
    "    #stage5\n",
    "    X = convolutional_block(X, f=3, filters=[512,512,2048], stage=5, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"c\")\n",
    "    \n",
    "    #均值池化层\n",
    "    X = AveragePooling2D(pool_size=(2,2),padding=\"same\")(X)\n",
    "    \n",
    "    #输出层\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation=\"softmax\", name=\"fc\"+str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    #创建模型\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"ResNet50\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "# 编译模型\n",
    "model = ResNet50(input_shape=(64,64,3),classes=6)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 获取数据\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = resnets_utils.load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig / 255.\n",
    "X_test = X_test_orig / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = resnets_utils.convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = resnets_utils.convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1080/1080 [==============================] - 216s 200ms/step - loss: 2.8581 - acc: 0.2528\n",
      "Epoch 2/2\n",
      "1080/1080 [==============================] - 336s 311ms/step - loss: 2.2588 - acc: 0.3685\n",
      "120/120 [==============================] - 10s 81ms/step\n",
      "Error = 11.039937019348145\n",
      "Accuracy = 0.17500000049670536\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(X_train,Y_train,epochs=2,batch_size=32)\n",
    "# 预测\n",
    "preds = model.evaluate(X_test,Y_test)\n",
    "\n",
    "print(\"Error = \" + str(preds[0]))\n",
    "print(\"Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
